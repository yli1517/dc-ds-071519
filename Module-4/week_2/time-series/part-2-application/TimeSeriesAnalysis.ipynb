{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "\n",
    "Imagine you are a financial analyst who wants to try and predict the price of a stock (Apple for example) for the next 10 days. What pieces of information(features) whould you think woudl be helpful in trying to forecast the price of the stock?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis: SARIMA Model\n",
    "\n",
    "## Agenda:\n",
    "- Working with time series data in pandas\n",
    "- Components of time series data\n",
    "- Stationarity\n",
    "- ACF and PACF\n",
    "- Fitting the Model\n",
    "- Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods of analysis and prediction dealing with data that is serially correlated is **Times Series Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series is a sequential set of data points, measured typically over successive times. It is mathematically defined as a set of vectors $$x_t$$ $$t = 0,1,2,...$$ where $t$ represents the time\n",
    "elapsed [21, 23, 31]. The variable $x_t$ is treated as a random variable. The measurements\n",
    "taken during an event in a time series are arranged in a proper chronological order.\n",
    "\n",
    "**Regression formula**:<br>\n",
    "$$ŷ_t   =   μ + ϕ_1 y_{t-1} +…+ ϕ_p y_{t-p} - θ_1e_{t-1} -…- θ_qe_{t-q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to modeling time series data\n",
    "![steps](./resources/flowchart.png)\n",
    "\n",
    "\n",
    "[Duke ARIMA documentaion](https://people.duke.edu/~rnau/411arim.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./resources/Sales_Data.csv', sep='\\t', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purchase_date']= pd.to_datetime(df['purchase_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['payment_amount']>100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales = df.groupby(['purchase_date'])['payment_amount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales.plot(figsize=(12,8), title= 'Test Prep Daily Sales', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales=daily_sales.resample('m').sum()\n",
    "monthly_sales  = pd.DataFrame(monthly_sales, columns=['payment_amount'])\n",
    "monthly_sales.plot(figsize=(12,8), title= 'Test Prep Sales: Monthly Sales', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of Time Series Data\n",
    "A time series in general is supposed to be affected by four main components, which can be separated from the observed data. These components are: *Trend, Cyclical, Seasonal and Irregular* components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trend** : The long term movement of a time series. For example, series relating to population growth, number of houses in a city etc. show upward trend.\n",
    "- **Seasonality** : Fluctuation in the data set that follow a regular pattern due to outside influences. For example sales of ice-cream increase in summer, or daily web traffic.\n",
    "- **Cyclical** : When data exhibit rises and falls that are not of fixed period.  Think of business cycles which usually last several years, but where the length of the current cycle is unknown beforehand.\n",
    "- **Irregrular**: Are caused by unpredictable influences, which are not regular and also do not repeat in a particular pattern. These variations are caused by incidences such as war, strike, earthquake, flood, revolution, etc. There is no defined statistical technique for measuring random fluctuations in a time series.\n",
    "\n",
    "\n",
    "*Note: Many people confuse cyclic behaviour with seasonal behaviour, but they are really quite different. If the fluctuations are not of fixed period then they are cyclic; if the period is unchanging and associated with some aspect of the calendar, then the pattern is seasonal.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(monthly_sales, freq=12)\n",
    "fig = plt.figure()\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can't we use linear regression techniques to forecast for times Seriese data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four principal assumptions which justify the use of linear regression models for purposes of inference or prediction:\n",
    "\n",
    "- Linearity and additivity of the relationship between dependent and independent variables:\n",
    "\n",
    "- Statistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data)\n",
    "\n",
    "-  Homoscedasticity (constant variance) of the errors\n",
    "\n",
    "- Normality of the error distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical stationarity: \n",
    "\n",
    "A **stationary time series** is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., \"stationarized\") through the use of mathematical transformations. A stationarized series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Homoscedasticity:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosntant Mean\n",
    "\n",
    "<img src='./resources/Mean_nonstationary.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant Variance\n",
    "\n",
    "<img src='./resources/Var_nonstationary.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocovariance that does not depend on time.\n",
    "\n",
    "<img src='./resources/Cov_nonstationary.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways to check for stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Plot rolling Statistics**\n",
    "\n",
    "    We can plot the moving average or moving variance and see if it varies with time. By moving average/variance I mean that at any instant ‘t’, we’ll take the average/variance of the last year, i.e. last 12 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Dickey-Fuller Test**:\n",
    "This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources on [Dicky-Fuller Test](https://nwfsc-timeseries.github.io/atsa-labs/sec-boxjenkins-aug-dickey-fuller.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#create a function that will help us to quickly \n",
    "def test_stationarity(timeseries, window):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=window).mean()\n",
    "    rolstd = timeseries.rolling(window=window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries.iloc[window:], color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the stationarity of the untransformed dataset\n",
    "test_stationarity(monthly_sales.payment_amount, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Stationarize time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log Transofrmation\n",
    "- Deflating the data\n",
    "- Differencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monthly_sales['natural_log'] = monthly_sales.payment_amount.apply(lambda x: np.log(x))  \n",
    "test_stationarity(monthly_sales.natural_log, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['first_difference'] = monthly_sales.payment_amount - monthly_sales.payment_amount.shift(1)  \n",
    "test_stationarity(monthly_sales.first_difference.dropna(inplace=False),12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonal Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['seasonal_difference'] = monthly_sales.payment_amount - monthly_sales.payment_amount.shift(12)  \n",
    "test_stationarity(monthly_sales.seasonal_difference.dropna(inplace=False), 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonal Difference of the First Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['seasonal_first_difference'] = monthly_sales.first_difference - monthly_sales.first_difference.shift(12)  \n",
    "test_stationarity(monthly_sales.seasonal_first_difference.dropna(inplace=False), 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Season Difference of the log first difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['log_first_difference'] = monthly_sales.natural_log - monthly_sales.natural_log.shift(1)  \n",
    "monthly_sales['log_seasonal_first_difference'] = monthly_sales.log_first_difference - monthly_sales.log_first_difference.shift(12)  \n",
    "\n",
    "test_stationarity(monthly_sales.log_first_difference.dropna(inplace=False), 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have to difference the differenced data (known as a second difference) to achieve stationary data. The number of times we have to difference our data is the order of differencing - we will use this information when building our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Integrated Moving Average  (ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive term\n",
    "\n",
    "If the series is stationary and autocorrelated, perhaps it can be predicted as a multiple of its own previous value, plus a constant.  The forecasting equation in this case is:\n",
    "\n",
    "\n",
    "$$Ŷt  =  μ  +  ϕ_1Y_{t-1}$$\n",
    "\n",
    "…which is Y regressed on itself lagged by one period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Moving Average term\n",
    "\n",
    "Another strategy for correcting autocorrelated errors is to use an average of the last few observations in order to filter out the noise and more accurately estimate the local mean. one of which is the so-called “error correction” form, in which the previous forecast is adjusted in the direction of the error it made:\n",
    "\n",
    "$$Ŷ_t   =  Ŷ_{t-1}  + αe_{t-1}$$  \n",
    "\n",
    "Because $e_{t-1} = Y_{t-1} - Ŷ_{t-1}$ by definition, this can be rewritten as:\n",
    "\n",
    "$$Ŷ_t   =  Y_{t-1}  - (1-α)e_{t-1}$$\n",
    "\n",
    "$$Ŷ_t =  Y_{t-1}  - θ_1e_{t-1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A seasonal ARIMA model is classified as an **ARIMA(p,d,q)x(P,D,Q)** model, \n",
    "\n",
    "    **p** = number of autoregressive (AR) terms \n",
    "    **d** = number of differences \n",
    "    **q** = number of moving average (MA) terms\n",
    "     \n",
    "    **P** = number of seasonal autoregressive (SAR) terms \n",
    "    **D** = number of seasonal differences \n",
    "    **Q** = number of seasonal moving average (SMA) terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we have achieved stationarity the next step in fitting a model to address any autocorrelation that remains in the differenced series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ACF/PACF charts and find optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial autocorrelation \n",
    "\n",
    "It is the correlation between a point and particular lag without the influence of itermediary lags. This helps us see the direct relationship between certain lags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF and PACF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "#plot the ACF\n",
    "fig = sm.graphics.tsa.plot_acf(monthly_sales.payment_amount.iloc[1:], lags=18, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "#plot the PACF\n",
    "fig = sm.graphics.tsa.plot_pacf(monthly_sales.payment_amount.iloc[1:], lags=18, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on first difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "#plot the ACF\n",
    "fig = sm.graphics.tsa.plot_acf(monthly_sales.first_difference.iloc[1:], lags=18, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "#plot the PACF\n",
    "fig = sm.graphics.tsa.plot_pacf(monthly_sales.first_difference.iloc[1:], lags=18, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on seasonal first difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "#plot the ACF\n",
    "fig = sm.graphics.tsa.plot_acf(monthly_sales.seasonal_first_difference.iloc[13:], lags=24, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "#plot the PACF\n",
    "fig = sm.graphics.tsa.plot_pacf(monthly_sales.seasonal_first_difference.iloc[13:], lags=24, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is still some correlation between the times series data, we want to include terms that will account for this correlation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinging AR and MA terms\n",
    "\n",
    "\n",
    "What’s the best way to correct for autocorrelation: \n",
    "- adding AR terms or \n",
    "- adding MA terms?  \n",
    "\n",
    "A rule-of-thumb for this situation, which will be discussed in more detail later on, is that positive autocorrelation is usually best treated by adding an AR term to the model and negative autocorrelation is usually best treated by adding an MA term. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./resources/armaguidelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules for Indentifying ARIMA Models\n",
    "\n",
    "https://people.duke.edu/~rnau/arimrule.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sm.tsa.statespace.SARIMAX()`\n",
    "[SARIMAX documentation](https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_sarimax_stata.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(monthly_sales['payment_amount'], trend='n', order=(0,1,1), seasonal_order=(0,1,1,12))\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Time Series Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use test train split when evaluating this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://people.duke.edu/~rnau/randwkf2.gif' width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The data in the **estimation period** are used to help select the model and to estimate its parameters. Forecasts made in this period are not completely \"honest\" because data on both sides of each observation are used to help determine the forecast. \n",
    "\n",
    "- The data in the **validation period** are held out during parameter estimation, and if you are careful you will also withhold these values during the exploratory phase of analysis when you select your model. One-step-ahead forecasts made in this period are often called backtests. Ideally, these are \"honest\" forecasts, however, if you test a great number of models and choose the model whose errors are smallest in the validation period, you may end up overfitting the data within the validation period as well as in the estimation period.\n",
    "\n",
    "- **Forecasts into the future** are \"true\" forecasts that are made for time periods beyond the end of the available data. For a model which is purely extrapolative in nature , it is possible to extend the forecasts an arbitrary number of periods into the future by \"bootstrapping\" the model: first a one-period-ahead forecast is made, then the one-period-ahead forecast is treated as a data point and the model is cranked ahead to produce a two-period-ahead forecast, and so on as far as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['forecast'] = res.predict(start = 54, end= 72, dynamic= False)  \n",
    "monthly_sales[['payment_amount', 'forecast']].plot(figsize=(16, 12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npredict =monthly_sales.payment_amount['2015'].shape[0]\n",
    "nobs = monthly_sales.payment_amount.shape[0]\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Graph\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "npre = 12\n",
    "ax.set(title='SALES', xlabel='Date', ylabel='dollars')\n",
    "ax.plot(monthly_sales.index[-npredict-npre+1:], monthly_sales.ix[-npredict-npre+1:, 'payment_amount'], 'o', label='Observed')\n",
    "ax.plot(monthly_sales.index[-npredict-npre+1:], monthly_sales.ix[-npredict-npre+1:, 'forecast'], 'g', label='Dynamic forecast')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error per prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(monthly_sales['2015']['payment_amount']- monthly_sales['2015']['forecast']).sum()/len(monthly_sales['2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's predict all of 2016\n",
    "\n",
    "In order for this to work, we need to extend our current data frame to account for the future time periods we will predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "start = datetime.datetime.strptime(\"2016-01-31\", \"%Y-%m-%d\")\n",
    "date_list = [start + relativedelta(months=x) for x in range(0,12)]\n",
    "future = pd.DataFrame(index=date_list, columns= monthly_sales.columns)\n",
    "monthly_sales = pd.concat([monthly_sales, future])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales['forecast'] = res.predict(start = 72, end = 84, dynamic= True)  \n",
    "monthly_sales[['payment_amount', 'forecast']].ix[-72:].plot(figsize=(12, 8)) \n",
    "plt.savefig('ts_predict_future.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to modeling time series data\n",
    "![steps](./resources/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application practice with dataset of trasit riders in Portland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./resources/portland-oregon-average-monthly-.csv', index_col=0)\n",
    "df.index.name=None\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(df.index[114], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other resources\n",
    "- [good kaggle tutorial](https://www.kaggle.com/poiupoiu/how-to-use-sarimax)\n",
    "- [standford paper](http://cs229.stanford.edu/proj2015/219_report.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:brown;\">  Introduction to neural network</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson plan \n",
    "1. Familiarity\n",
    "1. Applications\n",
    "1. Motivation\n",
    "  1. A piece of history \n",
    "  1. Connection to neurons \n",
    "1. The neural network graph\n",
    "  1. Computational graph with step function\n",
    "  1. Computational graph with sigmoid - Logitic Regression\n",
    "  1. The general Neural Network\n",
    "  1. Non linearity - XoR\n",
    "1. Optimization in NN aka backprop aka chain rule\n",
    "  1. Simple backprop example\n",
    "  1. Backprop simple rules\n",
    "1. Size constraint\n",
    "  1. Batch gradient descent\n",
    "  1. Epochs\n",
    "1. Takeaways\n",
    "\n",
    "### Next lectures: \n",
    "\n",
    "1. hands on exprience in running a neural network\n",
    "1. Different types of nets and tricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. It's quite familiar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/linear_vs_logistic_regression.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another way of writing it:\n",
    "![](img/log_reg_deriv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --> Logistic regression was our first Neural Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Applications (Let's see why the hype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the hype started by this <a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about applications: https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imprressive applications:\n",
    "- Image recognition: video, faces, neural style\n",
    "- Recommendation systems \n",
    "- Weather predictions\n",
    "- Alphago\n",
    "- Translations, speech recognition\n",
    "- Driverless cars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Videos:\n",
    "- <a href=\"https://www.youtube.com/watch?v=4eIBisqx9_g\">face recognition</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=gn4nRCC9TwQ\">walking robot </a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=8ypnLjwpzK8\">translations - open.ai</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. The imitation game - motivation I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are humans just very sophisticated machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/IPuswjolNFQze/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. A neuron (inside your brain) - motivation II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/kyR6tnuZItmec/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the brain as a function approximator trying to adjust weights and thresholds according to \n",
    "a desired result - i.e we are looking for a performence function aka loss function aka cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's very interesting and inspiring but only loosely related - be careful with analogies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what parts do we have:\n",
    "- inputs\n",
    "- weights \n",
    "- sum of weighted electrical currents\n",
    "- thresholds - should we activate or not \n",
    "- output\n",
    "- performance function\n",
    "    \n",
    "Don't these parts sound very fimiliar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Neural Network graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Simple computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comp_graph](./img/computational_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph where each node represents  a mathematical operation among all arrows pointing to the same node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Computational graph with step function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/step_func.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Notice that the step function is problematic for taking derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Computational graph with sigmoid function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log-reg](./img/log_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-linear function is called an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why sigmoid is not the best? meet RELU\n",
    "<a href=\"https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks\">Relu advantages</a>\n",
    "\n",
    "<a href=\"https://www.quora.com/What-are-the-advantages-of-using-Leaky-Rectified-Linear-Units-Leaky-ReLU-over-normal-ReLU-in-deep-learning\">Leaky relu advantages</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 General Neural network graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does the graphical representation of a neural network look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](./img/First_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">The procedure of flowing inputs to get output is called feed-forward </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why hidden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/Ice_cream_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why deep?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"deep networks have a hierarchical structure \n",
    "which makes them particularly well adapted to learn the hierarchies of knowledge \n",
    "that seem to be useful in solving real-world problems. Put more concretely, \n",
    "when attacking problems such as image recognition, \n",
    "it helps to use a system that understands not just individual pixels, \n",
    "but also increasingly more complex concepts: from edges to simple geometric shapes, \n",
    "all the way up through complex, multi-object scenes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. The strength of NN - XOR example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we classify correctly with linear separation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/XoR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no linear split that will separate the calsses correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class exercise (5 min)\n",
    "Let's try to solve it ourselves:\n",
    "- we have the following points [[1, 1], [0, 1], [1, 0], [0, 0]]\n",
    "- We will try to use the following formula: $f = W_2*max(0, W_1X + b)$\n",
    "- Take $W_1$ 2X2 matrix full with 1's \n",
    "- Take b to be [0, -1] \n",
    "- Take the max operation element-wise\n",
    "- take $W_2$ to be [1, -2]\n",
    "- use np.array and np.dot for matrix multiplication and np.maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More impressive example: <a href=\"http://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6&seed=0.49592&showTestData=false&discretize=false&percTrainData=60&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\">Playground</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended <a href=\"http://neuralnetworksanddeeplearning.com/chap4.html\">read</a>: Neural nets can approximate any function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How do we adjust weights & threshold? optimization = Backprop + update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Simple backprop example - some math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comp-graph](./img/computational_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop: \n",
    "\n",
    "- instead of taking the full derivative from the loss function to inputs:\n",
    "  - break the problem to small chuncks\n",
    "  - each time working on one edge and reusing previously calculated edges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p = x + y$ \n",
    "\n",
    "$g = p*z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial g}{\\partial p} = z$\n",
    "\n",
    "$\\frac{\\partial g}{\\partial z} = p$\n",
    "\n",
    "$\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial p} \\frac{\\partial p}{\\partial x}$\n",
    "\n",
    "$\\frac{\\partial g}{\\partial y} = \\frac{\\partial g}{\\partial p} \\frac{\\partial p}{\\partial y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Notice: we only calculate $\\frac{\\partial g}{\\partial p}$ once, then we can reuse it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget the parameter update in gradient descent is:\n",
    "    $W1 = W1 -\\epsilon * \\frac{\\partial f}{\\partial W1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ff-bb](./img/ff-bb.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Neural networks are data hungry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical image size is 64X64 = 4096 dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we want to use the element-wise max operator. \n",
    "Then the output will still be 4096 dimensional vector. \n",
    "This means $4096^2$ parameters for one image! If we have 10000 images we have \n",
    "1.6777216e+15 parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instead of calculating all observations' influence at once, choose randomly a small sample and update parameters accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular optimizers these days is called 'Adam', which generalizes from ordinary gradient descent by having individual and dynamic learning rates. This <a href=\"https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\">article</a> has a nice discussion of Adam.\n",
    "\n",
    "Other optimization algorithms:\n",
    "- adadelta\n",
    "- nestrov\n",
    "- momentum\n",
    "\n",
    "For the mathematical details, check out this <a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\">post</a>.\n",
    "\n",
    "Given that we are only passing some data points through at any one time, the question of when to update the weights becomes pressing. Standardly, we'd wait until we've passed all the data through before updating, but we might try updating after each batch (\"batch gradient descent\") or even after each point (\"stochastic gradient descent\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch means we are taking batches in a length of all observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we will run hundreds or thousands of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Terminology & concepts:\n",
    "    - Linear layers aka fully-connected aka dense layers\n",
    "    - Neuron: sum of weighted inputs \n",
    "    - Activation function: choose according to problem specifics. Common choices RELU, Leaky RELU (<a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\">Relu layers</a>), <a href=\"https://en.wikipedia.org/wiki/Hyperbolic_function\">tanh</a>\n",
    "    - Hidden layer: linear layer + activation function (we can add as many as we want)\n",
    "    - Feed-forward: push inputs through network to get the output\n",
    "    - Backprop: calculate the weights with respect to the loss by recurssion and reusing \n",
    "    - Batch gradient descent\n",
    "    - Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical neural net architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/Neural_network_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> Notice: it doesn't make sense to use 2 (or more) linear layers in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most commonly successful in (applications)\n",
    "    - NLP (natural language processing)\n",
    "    - Machine vision\n",
    "    - Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The bad\n",
    "    - Typically tons of data\n",
    "    - Tons of tunning \n",
    "    - Many things to adjust - parameters, layers, optimization techniques, learning rate, initializations\n",
    "    - Hard to interpret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The good\n",
    "    - After training very quick to produce an answer\n",
    "    - Amazing results if training correctly\n",
    "    - They are potentially very powerful function approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Tensorflow model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install progressbar2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape reddit page (takes a reddit .json url)\n",
    "# returns posts \n",
    "\n",
    "headers = {'User-Agent' : 'override this bad boy!'}\n",
    "\n",
    "def scraper_bike(url):\n",
    "    posts = []\n",
    "    after = {}\n",
    "\n",
    "    for page in progressbar(range(40)):\n",
    "        params = {'after' : after}\n",
    "        url = url\n",
    "        pagepull = requests.get(url = url, params = params, headers = headers)\n",
    "        page_dict = pagepull.json()\n",
    "        posts.extend(page_dict['data']['children'])\n",
    "        after = page_dict['data']['after']\n",
    "        time.sleep(.2)\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert posts to DataFrame - won't allow duplicate posts since unique id 'name' is set as index\n",
    "# Extract: name (as index) and subreddit, selftext, title (as columns)\n",
    "\n",
    "def posts_to_df(post_list):\n",
    "    i = 0\n",
    "    post_dict = {}\n",
    "    \n",
    "    for post in post_list:\n",
    "        ind = post_list[i]['data']\n",
    "        post_dict[ind['name']] = [ind['subreddit'], ind['title'], ind['selftext']]\n",
    "        i += 1\n",
    "\n",
    "    df_name = pd.DataFrame(post_dict)\n",
    "    df_name = df_name.T\n",
    "    df_name.columns = ['subreddit', 'title', 'selftext'] #'selftext'\n",
    "    \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes scraper function and url - outputs dataframe\n",
    "\n",
    "def scrape_to_df(scrape_func, url):\n",
    "    \n",
    "    return posts_to_df(scrape_func(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you want to scrape repeatedly over time and add to a csv\n",
    "# scrape, import csv, concat, drop duplicate, and output to csv\n",
    "# takes in scraper function, url, csv filename to import, csv filename to output\n",
    "# Outputs - Concatenated DataFrame as csv\n",
    "\n",
    "def scrape_add(scrape_func, url, import_file, export_file):\n",
    "    scrape_df = posts_to_df(scrape_func(url))\n",
    "    imported_df = pd.read_csv(import_file, index_col = 'Unnamed: 0')\n",
    "    concat_df = pd.concat([imported_df, scrape_df])\n",
    "    concat_df = concat_df[~concat_df.index.duplicated(keep='first')]\n",
    "    concat_df.to_csv(export_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:20 Time:  0:00:20\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:20 Time:  0:00:20\n"
     ]
    }
   ],
   "source": [
    "# Run this and comment out pd.read_csv lines in data cleaning / preprocessing to use freshly scraped data\n",
    "# You can also put in any 2 subreddits in as the URL and get results for those\n",
    "\n",
    "nfltest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nfl.json')\n",
    "nbatest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nba.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:22 Time:  0:00:22\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:20 Time:  0:00:20\n"
     ]
    }
   ],
   "source": [
    "politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(733, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbatest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(926, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfltest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t3_dbwb1y</th>\n",
       "      <td>nfl</td>\n",
       "      <td>I'm Kimberley Martin, senior NFL writer and Co...</td>\n",
       "      <td>Hi r/NFL, I'm Kimberley Martin, senior NFL wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbxfyg</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Official Week 4 /r/NFL Power Rankings</td>\n",
       "      <td>Good afternoon, r/nfl! We're through the first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dcajbu</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Tom Brady has been wearing the same shoulder p...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc44f6</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Jaguars] The Jaguars are giving out a bandana...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dca6d5</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Percy Harvin Says He Was High Every Game He Pl...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                              title  \\\n",
       "t3_dbwb1y       nfl  I'm Kimberley Martin, senior NFL writer and Co...   \n",
       "t3_dbxfyg       nfl              Official Week 4 /r/NFL Power Rankings   \n",
       "t3_dcajbu       nfl  Tom Brady has been wearing the same shoulder p...   \n",
       "t3_dc44f6       nfl  [Jaguars] The Jaguars are giving out a bandana...   \n",
       "t3_dca6d5       nfl  Percy Harvin Says He Was High Every Game He Pl...   \n",
       "\n",
       "                                                    selftext  \n",
       "t3_dbwb1y  Hi r/NFL, I'm Kimberley Martin, senior NFL wri...  \n",
       "t3_dbxfyg  Good afternoon, r/nfl! We're through the first...  \n",
       "t3_dcajbu                                                     \n",
       "t3_dc44f6                                                     \n",
       "t3_dca6d5                                                     "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfltest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These scrape_add functions add to already built csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/CollegeBasketball/new.json', 'NCAA_Posts_Update2.csv', 'NCAA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/AskScience/new.json', 'AskSci_Posts_Update2.csv', 'AskSci_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nba/new.json', 'NBA_Posts_Update2.csv', 'NBA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nfl/new.json', 'NFL_Posts_Update2.csv', 'NFL_Posts_Update3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "\n",
    "nfltest = nfltest.drop(columns = 'selftext')\n",
    "nbatest = nbatest.drop(columns = 'selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge subreddit data\n",
    "\n",
    "train = pd.concat([nfltest, nbatest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t3_dbwb1y</th>\n",
       "      <td>nfl</td>\n",
       "      <td>I'm Kimberley Martin, senior NFL writer and Cover 3 analyst for Yahoo! Sports. AMA!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbxfyg</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Official Week 4 /r/NFL Power Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dcajbu</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Tom Brady has been wearing the same shoulder pads since his freshman year at Michigan in 1995. They're older than 5 of his current team-mates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc44f6</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Jaguars] The Jaguars are giving out a bandana and a mustache to any fan who purchases tickets to the team's 2 home games this month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dca6d5</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Percy Harvin Says He Was High Every Game He Played</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "t3_dbwb1y       nfl   \n",
       "t3_dbxfyg       nfl   \n",
       "t3_dcajbu       nfl   \n",
       "t3_dc44f6       nfl   \n",
       "t3_dca6d5       nfl   \n",
       "\n",
       "                                                                                                                                                    title  \n",
       "t3_dbwb1y                                                             I'm Kimberley Martin, senior NFL writer and Cover 3 analyst for Yahoo! Sports. AMA!  \n",
       "t3_dbxfyg                                                                                                           Official Week 4 /r/NFL Power Rankings  \n",
       "t3_dcajbu  Tom Brady has been wearing the same shoulder pads since his freshman year at Michigan in 1995. They're older than 5 of his current team-mates.  \n",
       "t3_dc44f6            [Jaguars] The Jaguars are giving out a bandana and a mustache to any fan who purchases tickets to the team's 2 home games this month  \n",
       "t3_dca6d5                                                                                              Percy Harvin Says He Was High Every Game He Played  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize (grab only word characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'] = train['title'].map(lambda x: word_tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin list of tokenized words into single string for each row\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t3_dbwb1y                                                                 i m kimberley martin senior nfl writer and cover 3 analyst for yahoo sports ama\n",
       "t3_dbxfyg                                                                                                            official week 4 r nfl power rankings\n",
       "t3_dcajbu    tom brady has been wearing the same shoulder pads since his freshman year at michigan in 1995 they re older than 5 of his current team mates\n",
       "t3_dc44f6              jaguars the jaguars are giving out a bandana and a mustache to any fan who purchases tickets to the team s 2 home games this month\n",
       "t3_dca6d5                                                                                              percy harvin says he was high every game he played\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split and converting series to list of strings then to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    0.558168\n",
       "nba    0.441832\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline is\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sponsors feel nba india may be a slam dunk',\n",
       " 'los angeles clippers media day press conference live']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1244"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our CountVectorizer. This counts the number of appearances of all the words in our training data and\n",
    "# eliminates common english stop words. 5000 max features works well for our purposes (tested various numbers). Our\n",
    "# data is already preprocessed and tokenized manually earlier. ngram_range is 1,3, although all or nearly all our\n",
    "# features are single words\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit our training data and test data lists to our count_vectorizer\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to array\n",
    "\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1244, 5000), (415, 5000))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shapes\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I wanted check that the features corpus was as expected - removed print statement for readability\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '04',\n",
       " '10',\n",
       " '10 2014',\n",
       " '10 points',\n",
       " '10 reb',\n",
       " '10 yards',\n",
       " '100',\n",
       " '100 greatest',\n",
       " '100 greatest games',\n",
       " '1000',\n",
       " '101',\n",
       " '101 assists',\n",
       " '101 assists passing',\n",
       " '106',\n",
       " '11',\n",
       " '12',\n",
       " '12 games',\n",
       " '13',\n",
       " '13 weeks',\n",
       " '14',\n",
       " '14 17',\n",
       " '1400',\n",
       " '15',\n",
       " '150',\n",
       " '150 yard',\n",
       " '150 yard receiver',\n",
       " '150 yard rusher',\n",
       " '16',\n",
       " '16 carries',\n",
       " '16 games',\n",
       " '16 tds',\n",
       " '16m',\n",
       " '17',\n",
       " '17 straight',\n",
       " '17 straight games',\n",
       " '17 year',\n",
       " '179',\n",
       " '18',\n",
       " '18 65',\n",
       " '18 65 mph',\n",
       " '18 mph',\n",
       " '18 mph vs',\n",
       " '18 snapped',\n",
       " '18 snapped streaks',\n",
       " '19',\n",
       " '19 matt',\n",
       " '19 matt ryan',\n",
       " '19 season',\n",
       " '1950',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970 merger',\n",
       " '1998',\n",
       " '1999',\n",
       " '1st',\n",
       " '1st 4th',\n",
       " '1st place',\n",
       " '20',\n",
       " '20 years',\n",
       " '200',\n",
       " '200 snaps',\n",
       " '2000',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2008',\n",
       " '2010',\n",
       " '2010 finals',\n",
       " '2010s',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2018 19',\n",
       " '2018 2019',\n",
       " '2019',\n",
       " '2019 20',\n",
       " '2019 nba',\n",
       " '2019 nba media',\n",
       " '2019 nfl',\n",
       " '2020',\n",
       " '21',\n",
       " '22',\n",
       " '225',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '29 carries',\n",
       " '2nd',\n",
       " '2nd round',\n",
       " '2nd round pick',\n",
       " '2nd time',\n",
       " '30',\n",
       " '30 games',\n",
       " '30 years',\n",
       " '300',\n",
       " '300 yard',\n",
       " '300 yard passer',\n",
       " '30am',\n",
       " '31',\n",
       " '31st',\n",
       " '32',\n",
       " '32 games',\n",
       " '325',\n",
       " '325 second',\n",
       " '33',\n",
       " '33 yards',\n",
       " '33 yards drive',\n",
       " '34',\n",
       " '34 million',\n",
       " '35',\n",
       " '35 starter',\n",
       " '36',\n",
       " '37',\n",
       " '39',\n",
       " '3pt',\n",
       " '3rd',\n",
       " '3s',\n",
       " '40',\n",
       " '40 25',\n",
       " '40 years',\n",
       " '400',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '48',\n",
       " '49',\n",
       " '49ers',\n",
       " '4th',\n",
       " '4th quarter',\n",
       " '50',\n",
       " '500',\n",
       " '500 yards',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '549',\n",
       " '55',\n",
       " '56',\n",
       " '5th',\n",
       " '60',\n",
       " '60 games',\n",
       " '63',\n",
       " '63 yards',\n",
       " '65',\n",
       " '65 mph',\n",
       " '65 yards',\n",
       " '66',\n",
       " '69',\n",
       " '69 yards',\n",
       " '6th',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '76ers',\n",
       " '80',\n",
       " '81',\n",
       " '81 nfl',\n",
       " '81 nfl 100',\n",
       " '84',\n",
       " '84 points',\n",
       " '86',\n",
       " '88',\n",
       " '89',\n",
       " '8th',\n",
       " '90',\n",
       " '90 81',\n",
       " '90 81 nfl',\n",
       " '91',\n",
       " '92',\n",
       " '9th',\n",
       " 'aaron',\n",
       " 'aaron rodgers',\n",
       " 'ab',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'able hit',\n",
       " 'able hit deep',\n",
       " 'according',\n",
       " 'according source',\n",
       " 'accumulated',\n",
       " 'achieve',\n",
       " 'achilles',\n",
       " 'achilles injury',\n",
       " 'acl',\n",
       " 'act',\n",
       " 'acting']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression model\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1244, 5000), (1244,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape check\n",
    "\n",
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9887459807073955"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8843373493975903"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dataframe that matches features to coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'features': vectorizer.get_feature_names(),\n",
    "                        'coefs': coef_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>nba</td>\n",
       "      <td>-2.585113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>kawhi</td>\n",
       "      <td>-1.399729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>lakers</td>\n",
       "      <td>-1.279566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>basketball</td>\n",
       "      <td>-1.212108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>finals</td>\n",
       "      <td>-1.144695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        features     coefs\n",
       "1975         nba -2.585113\n",
       "1385       kawhi -1.399729\n",
       "1444      lakers -1.279566\n",
       "340   basketball -1.212108\n",
       "929       finals -1.144695"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by = ['coefs']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's throw out these unfair words and rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "extra_stopwords = ['nba', 'basketball', 'football', 'nfl']\n",
    "\n",
    "stopwords.update(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1244, 5000), (415, 5000))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=stopwords,\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9887459807073955"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8843373493975903"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>kawhi</td>\n",
       "      <td>-1.354888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>lakers</td>\n",
       "      <td>-1.288586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>finals</td>\n",
       "      <td>-1.273403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>lebron</td>\n",
       "      <td>-1.138784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>warriors</td>\n",
       "      <td>-1.064839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      features     coefs\n",
       "1449     kawhi -1.354888\n",
       "1509    lakers -1.288586\n",
       "961     finals -1.273403\n",
       "1559    lebron -1.138784\n",
       "4843  warriors -1.064839"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "coef_list = coef_list[0]\n",
    "\n",
    "coef_df = pd.DataFrame({'features': vectorizer.get_feature_names(),\n",
    "                        'coefs': coef_list})\n",
    "\n",
    "coef_df.sort_values(by=['coefs']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7807228915662651"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8843373493975903"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Matrix on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns=['predict_neg', 'predict_pos'],\n",
    "                    index = ['actual_neg', 'actual_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_neg</th>\n",
       "      <th>predict_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_neg</th>\n",
       "      <td>161</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_pos</th>\n",
       "      <td>26</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predict_neg  predict_pos\n",
       "actual_neg          161           22\n",
       "actual_pos           26          206"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking where our model failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({'y_actual' : y_test,\n",
    "             'y_predicted' : y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_df = comparison_df[comparison_df['y_actual'] != comparison_df['y_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "mismatch2_df = pd.concat([mismatch_df, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All incorrect predictions with titles\n",
    "\n",
    "mismatches = mismatch2_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_actual</th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t3_d9o0q3</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>did you know in 1954 the lakers and hawks played in an experimental regular season games where free throws weren t taken during the 1st and 3rd quarters but accumulated and taken at the end of the quarters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_d9yxo4</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>regular season all time scoring leader board offensive goats 1946 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_da2ict</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>kyrie explains in depth what went wrong in boston says he failed them as a leader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_da2vkd</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>feigen asked why he sought to get d antoni an extension but does not consider it necessary now fertitta said mike has representation i have my representation in daryl morey they could not come to terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_da3j4w</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>highlight pj tucker drops doncic with a between the legs crossover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dakzg1</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>who hit the most game winners at the buzzer regular season and playoffs combined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_daotbc</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>alex caruso on catching people off guard with athletic plays it s sneaky people say it s sneaky athleticism for the white guy it s all about sneaking up on them which makes it even more special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_davzae</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>kempski nfl should discipline andrew sendejo for reckless friendly fire shot on avonte maddox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dazc5e</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>compared the nfl regular season the nba reg season feels more like one long pre season what can be done to change that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_db3x9a</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>no stupid questions can a player receive a tech after the game ends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_db66i4</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>alternate angles of taysom hill trucking safety xavier woods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_db7d39</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>is isaiah thomas the greatest nba player 5 9 and under</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_db7vxc</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>can televised nba games ever be skipped over for another more interesting game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_db7yok</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>markelle fultz has been paid 465 640 per game played in the nba or 28 540 per minute played he is guaranteed a salary of 22 033 897 over the next two years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_db84oj</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>40 of the nfl is 2 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbavaq</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>the nfc looms like a few good teams might be left out of the playoffs who do you think they will be and what record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbay0a</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>peja stojaković with one of the best passes in nba history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbcexu</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>in 1985 86 the phoenix suns had an nba record streak of eight straight games with 30 or more personal fouls called against them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbezo1</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>stephen gilmore on antonio brown i hope he s alright he was a good teammate while he was here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbgimo</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>condotta seahawks have just announced former owner paul allen will be inducted into ring of honor thursday night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbgm5d</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>what other players should ve been suspended a whole season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbja1g</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>who s going to be the first coach to get fired this nba season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbjecd</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>rosenblatt duce staley on jordan howard he s a quiet guy and he just goes to work he picks up his lunch pail he picks up his hard hat and he just goes to work he s a good guy he s coachable he loves contact and he s physical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbjiy0</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>x post denver nuggets media day 2019 20 megathread for anyone that wants to check out everything that went down today in denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbkj3i</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>what star players are likely to retire prematurely in their career at the end of the season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbkvt1</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>diggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbkxx1</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>burns tony jefferson really was the best player on the browns sunday he gave them big play after big play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbltjj</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>highlight harden putting on a show westbrook loves it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbltx3</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>highlight harden to gordon who lobs it up for capela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbo1oj</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>1 4 of the way through season what record do you think your team will finish with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbotad</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>you need 1 1 highlight offensive or a defensive possession to earn a 10 mill contract which side of the ball you choosing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbqtnu</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>all defensive teams this century voting vs pipm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbt3hx</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>inman 49ers richard sherman on new california law allowing college athletes to make money off their likeness i hope it destroys the ncaa in general because i think it s corrupt and it s a bunch of people taking advantage of kids and doing it under a mask of fair play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbw4np</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>hoge danny trevathan on if he s concerned about roquan smith probably not as concerned as you guys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbwra0</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>who s a player that desperately needs to be coached up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbwvg3</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>kaboly tomlin about if his failed pi challenge will affect him down the road on what to challenge i have no idea what it is going to look like moving forward if anybody does i d appreciate it i don t think any of us have a feel on what that looks like and i am just being honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbymtx</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>pat mcafee nfl refs have to be better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dbzbu4</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>steelers depot mason rudolph juju has been an unbelievable leader for us even in the moments where he doesn t score he s positive as ever encouraging teammates he s a competitor i have to start getting him more touches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc0y0g</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>mike evans did lebron s celebration after a touchdown against the la rams and lebron tweeted about it turns out evans did it in front of kawhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc1ehy</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>craziest contracts right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc1naf</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>there are 53 players on your team 22 starters without looking could you name 25 of them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc2jpm</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>not nba but ernie johnson is calling a game right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc3qan</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>brett favre being brett favre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc43yr</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>jameis winston is a new man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc4l5u</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>question top 10 head coaches current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dc4quk</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>caputo based on the video mr kraft s guilt is a virtual certainty prosecutors in fl had until midnight to file their appeal after a county judge deemed the secretly recorded survillence video of robert kraft inadmissible in trial they filed the brief late tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dca6d5</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>percy harvin says he was high every game he played</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_dcaoo9</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>stiff arm hands to the face</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y_actual y_predicted  \\\n",
       "t3_d9o0q3      nba         nfl   \n",
       "t3_d9yxo4      nba         nfl   \n",
       "t3_da2ict      nba         nfl   \n",
       "t3_da2vkd      nba         nfl   \n",
       "t3_da3j4w      nba         nfl   \n",
       "t3_dakzg1      nba         nfl   \n",
       "t3_daotbc      nba         nfl   \n",
       "t3_davzae      nfl         nba   \n",
       "t3_dazc5e      nba         nfl   \n",
       "t3_db3x9a      nba         nfl   \n",
       "t3_db66i4      nfl         nba   \n",
       "t3_db7d39      nba         nfl   \n",
       "t3_db7vxc      nba         nfl   \n",
       "t3_db7yok      nba         nfl   \n",
       "t3_db84oj      nfl         nba   \n",
       "t3_dbavaq      nfl         nba   \n",
       "t3_dbay0a      nba         nfl   \n",
       "t3_dbcexu      nba         nfl   \n",
       "t3_dbezo1      nfl         nba   \n",
       "t3_dbgimo      nfl         nba   \n",
       "t3_dbgm5d      nfl         nba   \n",
       "t3_dbja1g      nba         nfl   \n",
       "t3_dbjecd      nfl         nba   \n",
       "t3_dbjiy0      nba         nfl   \n",
       "t3_dbkj3i      nfl         nba   \n",
       "t3_dbkvt1      nfl         nba   \n",
       "t3_dbkxx1      nfl         nba   \n",
       "t3_dbltjj      nba         nfl   \n",
       "t3_dbltx3      nba         nfl   \n",
       "t3_dbo1oj      nfl         nba   \n",
       "t3_dbotad      nba         nfl   \n",
       "t3_dbqtnu      nba         nfl   \n",
       "t3_dbt3hx      nfl         nba   \n",
       "t3_dbw4np      nfl         nba   \n",
       "t3_dbwra0      nfl         nba   \n",
       "t3_dbwvg3      nfl         nba   \n",
       "t3_dbymtx      nfl         nba   \n",
       "t3_dbzbu4      nfl         nba   \n",
       "t3_dc0y0g      nfl         nba   \n",
       "t3_dc1ehy      nba         nfl   \n",
       "t3_dc1naf      nfl         nba   \n",
       "t3_dc2jpm      nba         nfl   \n",
       "t3_dc3qan      nfl         nba   \n",
       "t3_dc43yr      nfl         nba   \n",
       "t3_dc4l5u      nfl         nba   \n",
       "t3_dc4quk      nfl         nba   \n",
       "t3_dca6d5      nfl         nba   \n",
       "t3_dcaoo9      nfl         nba   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                            title  \n",
       "t3_d9o0q3                                                                           did you know in 1954 the lakers and hawks played in an experimental regular season games where free throws weren t taken during the 1st and 3rd quarters but accumulated and taken at the end of the quarters  \n",
       "t3_d9yxo4                                                                                                                                                                                                                  regular season all time scoring leader board offensive goats 1946 2019  \n",
       "t3_da2ict                                                                                                                                                                                                       kyrie explains in depth what went wrong in boston says he failed them as a leader  \n",
       "t3_da2vkd                                                                               feigen asked why he sought to get d antoni an extension but does not consider it necessary now fertitta said mike has representation i have my representation in daryl morey they could not come to terms  \n",
       "t3_da3j4w                                                                                                                                                                                                                      highlight pj tucker drops doncic with a between the legs crossover  \n",
       "t3_dakzg1                                                                                                                                                                                                        who hit the most game winners at the buzzer regular season and playoffs combined  \n",
       "t3_daotbc                                                                                       alex caruso on catching people off guard with athletic plays it s sneaky people say it s sneaky athleticism for the white guy it s all about sneaking up on them which makes it even more special  \n",
       "t3_davzae                                                                                                                                                                                           kempski nfl should discipline andrew sendejo for reckless friendly fire shot on avonte maddox  \n",
       "t3_dazc5e                                                                                                                                                                  compared the nfl regular season the nba reg season feels more like one long pre season what can be done to change that  \n",
       "t3_db3x9a                                                                                                                                                                                                                     no stupid questions can a player receive a tech after the game ends  \n",
       "t3_db66i4                                                                                                                                                                                                                            alternate angles of taysom hill trucking safety xavier woods  \n",
       "t3_db7d39                                                                                                                                                                                                                                  is isaiah thomas the greatest nba player 5 9 and under  \n",
       "t3_db7vxc                                                                                                                                                                                                          can televised nba games ever be skipped over for another more interesting game  \n",
       "t3_db7yok                                                                                                                             markelle fultz has been paid 465 640 per game played in the nba or 28 540 per minute played he is guaranteed a salary of 22 033 897 over the next two years  \n",
       "t3_db84oj                                                                                                                                                                                                                                                                    40 of the nfl is 2 2  \n",
       "t3_dbavaq                                                                                                                                                                     the nfc looms like a few good teams might be left out of the playoffs who do you think they will be and what record  \n",
       "t3_dbay0a                                                                                                                                                                                                                              peja stojaković with one of the best passes in nba history  \n",
       "t3_dbcexu                                                                                                                                                         in 1985 86 the phoenix suns had an nba record streak of eight straight games with 30 or more personal fouls called against them  \n",
       "t3_dbezo1                                                                                                                                                                                           stephen gilmore on antonio brown i hope he s alright he was a good teammate while he was here  \n",
       "t3_dbgimo                                                                                                                                                                        condotta seahawks have just announced former owner paul allen will be inducted into ring of honor thursday night  \n",
       "t3_dbgm5d                                                                                                                                                                                                                              what other players should ve been suspended a whole season  \n",
       "t3_dbja1g                                                                                                                                                                                                                          who s going to be the first coach to get fired this nba season  \n",
       "t3_dbjecd                                                        rosenblatt duce staley on jordan howard he s a quiet guy and he just goes to work he picks up his lunch pail he picks up his hard hat and he just goes to work he s a good guy he s coachable he loves contact and he s physical  \n",
       "t3_dbjiy0                                                                                                                                                         x post denver nuggets media day 2019 20 megathread for anyone that wants to check out everything that went down today in denver  \n",
       "t3_dbkj3i                                                                                                                                                                                             what star players are likely to retire prematurely in their career at the end of the season  \n",
       "t3_dbkvt1                                                                                                                                                                                                                                                                                   diggs  \n",
       "t3_dbkxx1                                                                                                                                                                               burns tony jefferson really was the best player on the browns sunday he gave them big play after big play  \n",
       "t3_dbltjj                                                                                                                                                                                                                                   highlight harden putting on a show westbrook loves it  \n",
       "t3_dbltx3                                                                                                                                                                                                                                    highlight harden to gordon who lobs it up for capela  \n",
       "t3_dbo1oj                                                                                                                                                                                                       1 4 of the way through season what record do you think your team will finish with  \n",
       "t3_dbotad                                                                                                                                                               you need 1 1 highlight offensive or a defensive possession to earn a 10 mill contract which side of the ball you choosing  \n",
       "t3_dbqtnu                                                                                                                                                                                                                                         all defensive teams this century voting vs pipm  \n",
       "t3_dbt3hx             inman 49ers richard sherman on new california law allowing college athletes to make money off their likeness i hope it destroys the ncaa in general because i think it s corrupt and it s a bunch of people taking advantage of kids and doing it under a mask of fair play  \n",
       "t3_dbw4np                                                                                                                                                                                      hoge danny trevathan on if he s concerned about roquan smith probably not as concerned as you guys  \n",
       "t3_dbwra0                                                                                                                                                                                                                                  who s a player that desperately needs to be coached up  \n",
       "t3_dbwvg3  kaboly tomlin about if his failed pi challenge will affect him down the road on what to challenge i have no idea what it is going to look like moving forward if anybody does i d appreciate it i don t think any of us have a feel on what that looks like and i am just being honest  \n",
       "t3_dbymtx                                                                                                                                                                                                                                                   pat mcafee nfl refs have to be better  \n",
       "t3_dbzbu4                                                              steelers depot mason rudolph juju has been an unbelievable leader for us even in the moments where he doesn t score he s positive as ever encouraging teammates he s a competitor i have to start getting him more touches  \n",
       "t3_dc0y0g                                                                                                                                          mike evans did lebron s celebration after a touchdown against the la rams and lebron tweeted about it turns out evans did it in front of kawhi  \n",
       "t3_dc1ehy                                                                                                                                                                                                                                                            craziest contracts right now  \n",
       "t3_dc1naf                                                                                                                                                                                                 there are 53 players on your team 22 starters without looking could you name 25 of them  \n",
       "t3_dc2jpm                                                                                                                                                                                                                                   not nba but ernie johnson is calling a game right now  \n",
       "t3_dc3qan                                                                                                                                                                                                                                                           brett favre being brett favre  \n",
       "t3_dc43yr                                                                                                                                                                                                                                                             jameis winston is a new man  \n",
       "t3_dc4l5u                                                                                                                                                                                                                                                    question top 10 head coaches current  \n",
       "t3_dc4quk                 caputo based on the video mr kraft s guilt is a virtual certainty prosecutors in fl had until midnight to file their appeal after a county judge deemed the secretly recorded survillence video of robert kraft inadmissible in trial they filed the brief late tonight  \n",
       "t3_dca6d5                                                                                                                                                                                                                                      percy harvin says he was high every game he played  \n",
       "t3_dcaoo9                                                                                                                                                                                                                                                             stiff arm hands to the face  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency / Inverse Document Frequency\n",
    "\n",
    "TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(w) = log_e(Total number of documents / Number of documents with term w in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                            tokenizer=None,\n",
    "                            preprocessor=None,\n",
    "                            stop_words=list(s_words).extend(['nba', 'nfl', 'basketball', 'football']),\n",
    "                            max_features=5000,\n",
    "                            ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1244, 5000), (415, 5000))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features = tfidf_vec.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = tfidf_vec.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9887459807073955"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867469879518072"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try on some other subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([politics_test, conservative_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "# conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_test = politics_test.drop(columns = 'selftext')\n",
    "conservative_test = conservative_test.drop(columns = 'selftext')\n",
    "\n",
    "train = pd.concat([politics_test, conservative_test])\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)\n",
    "    \n",
    "    \n",
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "s_words = set(stopwords.words('english') + stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'd',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'donde',\n",
       " 'down',\n",
       " 'durante',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'further',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'hasta',\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mis',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'muy',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'not',\n",
       " 'now',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'os',\n",
       " 'other',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'so',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'some',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'such',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 't',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'to',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'too',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'under',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = s_words,\n",
    "                             max_features = 5000,\n",
    "                             ngram_range = (1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1387, 5000), (1387,))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9798125450612833"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883369330453563"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "\n",
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>biden</td>\n",
       "      <td>-1.224174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>dems</td>\n",
       "      <td>-1.110635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>hillary</td>\n",
       "      <td>-1.032156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>schiff</td>\n",
       "      <td>-0.979207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>pelosi</td>\n",
       "      <td>-0.957020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>democrats subpoena</td>\n",
       "      <td>-0.880528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>illegal</td>\n",
       "      <td>-0.857690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>complaint</td>\n",
       "      <td>-0.857276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>california</td>\n",
       "      <td>-0.854052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>deduction</td>\n",
       "      <td>-0.849851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>democratic party</td>\n",
       "      <td>-0.833826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>left</td>\n",
       "      <td>-0.812431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>bring</td>\n",
       "      <td>-0.803299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>liberals</td>\n",
       "      <td>-0.790970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>-0.786948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>aoc</td>\n",
       "      <td>-0.759658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>reporter</td>\n",
       "      <td>-0.756322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>impeached</td>\n",
       "      <td>-0.723495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465</th>\n",
       "      <td>support impeachment inquiry</td>\n",
       "      <td>-0.707674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>agenda</td>\n",
       "      <td>-0.681980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>let</td>\n",
       "      <td>-0.680690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>dismisses</td>\n",
       "      <td>-0.677636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>bombshell</td>\n",
       "      <td>-0.677548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>trump impeachment</td>\n",
       "      <td>-0.676768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>made</td>\n",
       "      <td>-0.669220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>leftists</td>\n",
       "      <td>-0.668252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>racist</td>\n",
       "      <td>-0.665106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>individual</td>\n",
       "      <td>-0.660239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4604</th>\n",
       "      <td>transcript</td>\n",
       "      <td>-0.656393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>john</td>\n",
       "      <td>-0.640761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>grassley</td>\n",
       "      <td>0.709151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>mueller</td>\n",
       "      <td>0.713502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>military</td>\n",
       "      <td>0.716202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>battle</td>\n",
       "      <td>0.718560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>one</td>\n",
       "      <td>0.725342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>giuliani</td>\n",
       "      <td>0.727195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>targets</td>\n",
       "      <td>0.746292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>race ukraine</td>\n",
       "      <td>0.758945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4787</th>\n",
       "      <td>undocumented</td>\n",
       "      <td>0.769655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>facebook</td>\n",
       "      <td>0.775247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>congress</td>\n",
       "      <td>0.790193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>fair</td>\n",
       "      <td>0.792913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>collins</td>\n",
       "      <td>0.801953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>says trump</td>\n",
       "      <td>0.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>marijuana</td>\n",
       "      <td>0.815060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>bernie</td>\n",
       "      <td>0.848645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>conspiracy</td>\n",
       "      <td>0.889931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3065</th>\n",
       "      <td>putin</td>\n",
       "      <td>0.896114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>zuckerberg</td>\n",
       "      <td>0.917302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>republicans</td>\n",
       "      <td>0.920342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>yang</td>\n",
       "      <td>0.931415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>manufacturing</td>\n",
       "      <td>0.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>poll</td>\n",
       "      <td>0.974259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>trade</td>\n",
       "      <td>1.026015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>barr</td>\n",
       "      <td>1.046415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>us</td>\n",
       "      <td>1.107119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>foreign</td>\n",
       "      <td>1.109953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>sanders</td>\n",
       "      <td>1.278159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4621</th>\n",
       "      <td>trump</td>\n",
       "      <td>1.557057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>pompeo</td>\n",
       "      <td>2.019466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         features     coefs\n",
       "304                         biden -1.224174\n",
       "750                          dems -1.110635\n",
       "1217                      hillary -1.032156\n",
       "4261                       schiff -0.979207\n",
       "2524                       pelosi -0.957020\n",
       "743            democrats subpoena -0.880528\n",
       "1284                      illegal -0.857690\n",
       "596                     complaint -0.857276\n",
       "410                    california -0.854052\n",
       "704                     deduction -0.849851\n",
       "726              democratic party -0.833826\n",
       "1523                         left -0.812431\n",
       "391                         bring -0.803299\n",
       "1545                     liberals -0.790970\n",
       "1529                   legitimate -0.786948\n",
       "186                           aoc -0.759658\n",
       "3818                     reporter -0.756322\n",
       "1300                    impeached -0.723495\n",
       "4465  support impeachment inquiry -0.707674\n",
       "117                        agenda -0.681980\n",
       "1533                          let -0.680690\n",
       "801                     dismisses -0.677636\n",
       "362                     bombshell -0.677548\n",
       "4661            trump impeachment -0.676768\n",
       "1599                         made -0.669220\n",
       "1526                     leftists -0.668252\n",
       "3210                       racist -0.665106\n",
       "1340                   individual -0.660239\n",
       "4604                   transcript -0.656393\n",
       "1418                         john -0.640761\n",
       "...                           ...       ...\n",
       "1139                     grassley  0.709151\n",
       "1776                      mueller  0.713502\n",
       "1730                     military  0.716202\n",
       "277                        battle  0.718560\n",
       "2476                          one  0.725342\n",
       "1099                     giuliani  0.727195\n",
       "4505                      targets  0.746292\n",
       "3191                 race ukraine  0.758945\n",
       "4787                 undocumented  0.769655\n",
       "942                      facebook  0.775247\n",
       "615                      congress  0.790193\n",
       "953                          fair  0.792913\n",
       "570                       collins  0.801953\n",
       "4256                   says trump  0.810400\n",
       "1621                    marijuana  0.815060\n",
       "287                        bernie  0.848645\n",
       "623                    conspiracy  0.889931\n",
       "3065                        putin  0.896114\n",
       "4997                   zuckerberg  0.917302\n",
       "3873                  republicans  0.920342\n",
       "4979                         yang  0.931415\n",
       "1614                manufacturing  0.945400\n",
       "2583                         poll  0.974259\n",
       "4595                        trade  1.026015\n",
       "266                          barr  1.046415\n",
       "4803                           us  1.107119\n",
       "1030                      foreign  1.109953\n",
       "4226                      sanders  1.278159\n",
       "4621                        trump  1.557057\n",
       "2589                       pompeo  2.019466\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({'features': vectorizer.get_feature_names(),\n",
    "                        'coefs': coef_list})\n",
    "\n",
    "coef_df.sort_values(by=['coefs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Conservative', 'politics'], dtype=object)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9691784473039958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1000)\n",
    "pca.fit(train_data_features)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "# print(pca.singular_values_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = pca.transform(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_test = pca.transform(test_data_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509733237202596"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883369330453563"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "\n",
    "![miners](img/text-miners.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is text mining different? What is text?\n",
    "\n",
    "- Order the words from **SMALLEST** to **LARGEST** units\n",
    " - character\n",
    " - corpora\n",
    " - sentence\n",
    " - word\n",
    " - corpus\n",
    " - paragraph\n",
    " - document\n",
    "\n",
    "(after it is all organized)\n",
    "\n",
    "- Any disagreements about the terms used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1, 7, 3, 2, 6, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## start small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is a token? split them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = \"Here is a sentence. Or two, I don't think there will be more.\"\n",
    "token_test_2 = \"i thought this sentence was good.\"\n",
    "token_test_3 = \"Here's a sentence... maybe two. Depending on how you like to count!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-df116956d950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmake_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-df116956d950>\u001b[0m in \u001b[0;36mmake_sentences\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's tokenize a document... into sentences\n",
    "def make_sentences(doc):\n",
    "    punct = ['.', '?', '!']\n",
    "    sentences = [doc]\n",
    "    for p in punct:\n",
    "        for s in sentences:\n",
    "            sentences.extend(doc.split(p))\n",
    "    return sentences\n",
    "\n",
    "make_sentences(token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's tokenize a document into words\n",
    "# with these 3 test cases what would you look out for?\n",
    "def tokenize_it(doc):\n",
    "    pass\n",
    "\n",
    "tokenize_it(token_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New library!\n",
    "\n",
    "while we have seen language processing tools in spark, NLTK is its own python library. And of course, it has its own [documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, word_tokenize\n",
    "import string\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yl/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorph = urllib.request.urlopen('http://www.gutenberg.org/cache/epub/5200/pg5200.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorph_st = metamorph.decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\r\n",
      "Translated by David Wyllie.\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.net\r\n",
      "\r\n",
      "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\r\n",
      "**     Please follow the copyright guidelines in this file.     **\r\n",
      "\r\n",
      "\r\n",
      "Title: Metamorphosis\r\n",
      "\r\n",
      "Author: Franz Kafka\r\n",
      "\r\n",
      "Translator: David Wyllie\r\n",
      "\r\n",
      "Release Date: August 16, 2005 [EBook #5200]\r\n",
      "First posted: May 13, 2002\r\n",
      "Last updated: May 20, 2012\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Copyright (C) 2002 David Wyllie.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  Metamorphosis\r\n",
      "  Franz Kafka\r\n",
      "\r\n",
      "Translated by David Wyllie\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "I\r\n",
      "\r\n",
      "\r\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\r\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "print(metamorph_st[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# response = requests.get('http://www.gutenberg.org/cache/epub/5200/pg5200.txt')\n",
    "# metamorph = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r\\nTranslated by David Wyllie.\\r\\n\\r\\nThis e'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metamorph[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your article here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', 'Details', 'Below', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "metamorph_tokens_raw = nltk.regexp_tokenize(metamorph_st, pattern)\n",
    "print(metamorph_tokens_raw[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'net', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this']\n"
     ]
    }
   ],
   "source": [
    "metamorph_tokens = [i.lower() for i in metamorph_tokens_raw]\n",
    "print(metamorph_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'use', 'terms', 'project', 'license', 'included', 'online', 'www', 'net', 'copyrighted', 'project', 'details', 'please', 'follow', 'copyright', 'guidelines', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'project', 'metamorphosis', 'copyright', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english') + ['would', 'gutenberg', 'ebook'])\n",
    "metamorph_tokens_stopped = [w for w in metamorph_tokens if not w in stop_words]\n",
    "print(metamorph_tokens_stopped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Porter Stemmer \n",
    "![porter](https://cdn.homebrewersassociation.org/wp-content/uploads/Baltic_Porter_Feature-600x800.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = ['caresses', 'flies', 'flying', 'dies', 'mules', 'denied', 'deny'\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli fli die mule deni denydi agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "singles = [stemmer.stem(e) for e in example]\n",
    "print(*singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Snowball Stemmer\n",
    "![snowball](https://localtvwiti.files.wordpress.com/2018/08/gettyimages-936380496.jpg?quality=85&strip=all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "finnish\n",
      "french\n",
      "german\n",
      "hungarian\n",
      "italian\n",
      "norwegian\n",
      "porter\n",
      "portuguese\n",
      "romanian\n",
      "russian\n",
      "spanish\n",
      "swedish\n"
     ]
    }
   ],
   "source": [
    "print(*SnowballStemmer.languages, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball vs. Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Snowball on metamorphesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'use', 'term', 'project', 'licens', 'includ', 'onlin', 'www', 'net', 'copyright', 'project', 'detail', 'pleas', 'follow', 'copyright', 'guidelin', 'file', 'titl', 'metamorphosi', 'author', 'franz', 'kafka', 'translat', 'david', 'wylli', 'releas', 'date', 'august', 'first', 'post', 'may', 'last', 'updat', 'may', 'languag', 'english', 'start', 'project', 'metamorphosi', 'copyright', 'c', 'david', 'wylli', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armour', 'like', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slight', 'dome', 'divid', 'arch', 'stiff', 'section', 'bed', 'hard', 'abl', 'cover', 'seem']\n"
     ]
    }
   ],
   "source": [
    "meta_stemmed = [stemmer.stem(word) for word in metamorph_tokens_stopped]\n",
    "print(meta_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a short list of additional considerations when cleaning text:\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_freqdist = FreqDist(meta_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gregor', 298),\n",
       " ('room', 133),\n",
       " ('could', 120),\n",
       " ('work', 114),\n",
       " ('even', 104),\n",
       " ('father', 102),\n",
       " ('sister', 101),\n",
       " ('door', 97),\n",
       " ('mother', 90),\n",
       " ('project', 88),\n",
       " ('back', 83),\n",
       " ('one', 76),\n",
       " ('time', 74),\n",
       " ('way', 66),\n",
       " ('look', 61),\n",
       " ('tm', 57),\n",
       " ('open', 56),\n",
       " ('use', 55),\n",
       " ('get', 52),\n",
       " ('said', 51),\n",
       " ('littl', 49),\n",
       " ('go', 49),\n",
       " ('without', 47),\n",
       " ('first', 45),\n",
       " ('still', 45),\n",
       " ('want', 44),\n",
       " ('like', 43),\n",
       " ('see', 42),\n",
       " ('hand', 41),\n",
       " ('made', 40),\n",
       " ('make', 40),\n",
       " ('head', 39),\n",
       " ('much', 39),\n",
       " ('come', 39),\n",
       " ('day', 38),\n",
       " ('thing', 38),\n",
       " ('move', 38),\n",
       " ('chief', 38),\n",
       " ('thought', 37),\n",
       " ('clerk', 37),\n",
       " ('turn', 36),\n",
       " ('away', 35),\n",
       " ('samsa', 34),\n",
       " ('let', 33),\n",
       " ('bed', 32),\n",
       " ('well', 32),\n",
       " ('went', 32),\n",
       " ('famili', 32),\n",
       " ('left', 32),\n",
       " ('seem', 31)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "meta_freqdist.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yl/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('flies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "## this step happens after we account for stopwords and lemmas; depending on the library...\n",
    "* we make a **Count Vector**, which is the formal term for a **bag of words**\n",
    "* we use vectors to pass text into machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the CountVectorizer method on 'basic_example'\n",
    "basic_example = ['The Data Scientist wants to train a machine to train machine learning models.']\n",
    "cv = CountVectorizer()\n",
    "cv.fit(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what info can we get from cv?\n",
    "# hint -- look at the docs again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'models',\n",
       " 'scientist',\n",
       " 'the',\n",
       " 'to',\n",
       " 'train',\n",
       " 'wants']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5,\n",
       " 'data': 0,\n",
       " 'scientist': 4,\n",
       " 'wants': 8,\n",
       " 'to': 6,\n",
       " 'train': 7,\n",
       " 'machine': 2,\n",
       " 'learning': 1,\n",
       " 'models': 3}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 2, 1, 1, 1, 2, 2, 1]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(basic_example).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         1        2       1          1    1   2      2      1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(cv.transform(basic_example).toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization allows us to compare two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to help see what's happening\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the CountVectorizer on the 'basic_example', now we transform 'basic_example'\n",
    "example_vector_doc_1 = cv.transform(basic_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# what is the type\n",
    "\n",
    "print(type(example_vector_doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t2\n",
      "  (0, 7)\t2\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# what does it look like\n",
    "\n",
    "print(example_vector_doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         1        2       1          1    1   2      2      1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "example_vector_df = pd.DataFrame(example_vector_doc_1.toarray(), columns=cv.get_feature_names())\n",
    "example_vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>scientist</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  machine  models  scientist  the  to  train  wants\n",
       "0     1         0        0       0          1    2   0      0      0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we compare new text to the CountVectorizer fit on 'basic_example'\n",
    "new_text = ['the data scientist plotted the residual error of her model']\n",
    "new_data = cv.transform(new_text)\n",
    "new_count = pd.DataFrame(new_data.toarray(),columns=cv.get_feature_names())\n",
    "new_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this the object 'sentences' becomes the corpus\n",
    "sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "             'the data scientist plotted the residual error of her model in her analysis',\n",
    "             'Her analysis was so good, she won a Kaggle competition.',\n",
    "             'The machine gained sentience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to the docs for count vectorizer, how would we use an ngram\n",
    "# pro tip -- include stop words\n",
    "bigrams = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x26 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vector = bigrams.fit_transform(sentences)\n",
    "bigram_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 features for this corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['analysis',\n",
       " 'competition',\n",
       " 'data',\n",
       " 'error',\n",
       " 'gained',\n",
       " 'good',\n",
       " 'her',\n",
       " 'in',\n",
       " 'kaggle',\n",
       " 'learning']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {str(len(bigrams.get_feature_names()))} features for this corpus')\n",
    "bigrams.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>gained</th>\n",
       "      <th>good</th>\n",
       "      <th>her</th>\n",
       "      <th>in</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>...</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sentience</th>\n",
       "      <th>she</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>was</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition  data  error  gained  good  her  in  kaggle  \\\n",
       "0         0            0     1      0       0     0    0   0       0   \n",
       "1         1            0     1      1       0     0    2   1       0   \n",
       "2         1            1     0      0       0     1    1   0       1   \n",
       "3         0            0     0      0       1     0    0   0       0   \n",
       "\n",
       "   learning  ...  scientist  sentience  she  so  the  to  train  wants  was  \\\n",
       "0         1  ...          1          0    0   0    1   2      2      1    0   \n",
       "1         0  ...          1          0    0   0    2   0      0      0    0   \n",
       "2         0  ...          0          0    1   1    0   0      0      0    1   \n",
       "3         0  ...          0          1    0   0    1   0      0      0    0   \n",
       "\n",
       "   won  \n",
       "0    0  \n",
       "1    0  \n",
       "2    1  \n",
       "3    0  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's visualize it\n",
    "bigram_df = pd.DataFrame(bigram_vector.toarray(), columns=bigrams.get_feature_names())\n",
    "bigram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_sentences = ['The Data Scientist wants to train a machine to train machine learning models.',\n",
    "                    'the data scientist plotted the residual error of her model in her analysis',\n",
    "                    'Her analysis was so good, she won a Kaggle competition.',\n",
    "                    'The machine gained sentiance']\n",
    "# take out stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "# fit transform the sentences\n",
    "tfidf_sentences = tfidf.fit_transform(tf_idf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it\n",
    "tfidf_df = pd.DataFrame(tfidf_sentences.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>gained</th>\n",
       "      <th>good</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>plotted</th>\n",
       "      <th>residual</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sentiance</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.610575</td>\n",
       "      <td>0.305288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366739</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition      data     error    gained      good    kaggle  \\\n",
       "0  0.000000     0.000000  0.240692  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.325557     0.000000  0.325557  0.412928  0.000000  0.000000  0.000000   \n",
       "2  0.366739     0.465162  0.000000  0.000000  0.000000  0.465162  0.465162   \n",
       "3  0.000000     0.000000  0.000000  0.000000  0.617614  0.000000  0.000000   \n",
       "\n",
       "   learning   machine     model    models   plotted  residual  scientist  \\\n",
       "0  0.305288  0.481384  0.000000  0.305288  0.000000  0.000000   0.240692   \n",
       "1  0.000000  0.000000  0.412928  0.000000  0.412928  0.412928   0.325557   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.486934  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "   sentiance     train     wants       won  \n",
       "0   0.000000  0.610575  0.305288  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.000000  0.465162  \n",
       "3   0.617614  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>gained</th>\n",
       "      <th>good</th>\n",
       "      <th>her</th>\n",
       "      <th>in</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>...</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sentience</th>\n",
       "      <th>she</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>was</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition  data  error  gained  good  her  in  kaggle  \\\n",
       "0         0            0     1      0       0     0    0   0       0   \n",
       "1         1            0     1      1       0     0    2   1       0   \n",
       "2         1            1     0      0       0     1    1   0       1   \n",
       "3         0            0     0      0       1     0    0   0       0   \n",
       "\n",
       "   learning  ...  scientist  sentience  she  so  the  to  train  wants  was  \\\n",
       "0         1  ...          1          0    0   0    1   2      2      1    0   \n",
       "1         0  ...          1          0    0   0    2   0      0      0    0   \n",
       "2         0  ...          0          0    1   1    0   0      0      0    1   \n",
       "3         0  ...          0          1    0   0    1   0      0      0    0   \n",
       "\n",
       "   won  \n",
       "0    0  \n",
       "1    0  \n",
       "2    1  \n",
       "3    0  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to bigrams\n",
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test out our TfidfVectorizer\n",
    "test_tdidf = tfidf.transform(['this is a test document','look at me I am a test document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x18 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a vector\n",
    "test_tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>competition</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>gained</th>\n",
       "      <th>good</th>\n",
       "      <th>kaggle</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>model</th>\n",
       "      <th>models</th>\n",
       "      <th>plotted</th>\n",
       "      <th>residual</th>\n",
       "      <th>scientist</th>\n",
       "      <th>sentiance</th>\n",
       "      <th>train</th>\n",
       "      <th>wants</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  competition  data  error  gained  good  kaggle  learning  \\\n",
       "0       0.0          0.0   0.0    0.0     0.0   0.0     0.0       0.0   \n",
       "1       0.0          0.0   0.0    0.0     0.0   0.0     0.0       0.0   \n",
       "\n",
       "   machine  model  models  plotted  residual  scientist  sentiance  train  \\\n",
       "0      0.0    0.0     0.0      0.0       0.0        0.0        0.0    0.0   \n",
       "1      0.0    0.0     0.0      0.0       0.0        0.0        0.0    0.0   \n",
       "\n",
       "   wants  won  \n",
       "0    0.0  0.0  \n",
       "1    0.0  0.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf_df = pd.DataFrame(test_tdidf.toarray(), columns=tfidf.get_feature_names())\n",
    "test_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Similarity Between Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell how similar two documents are to one another, normalizing for size, by taking the cosine similarity of the two. \n",
    "\n",
    "This number will range from [0,1], with 0 being not similar whatsoever, and 1 being the exact same. A potential application of cosine similarity is a basic recommendation engine. If you wanted to recommend articles that are most similar to other articles, you could talk the cosine similarity of all articles and return the highest one.\n",
    "\n",
    "<img src=\"./img/better_cos_similarity.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = CountVectorizer()\n",
    "sunday_afternoon = ['I ate a burger at burger queen and it was very good.',\n",
    "                    'I ate a hot dog at burger prince and it was bad',\n",
    "                    'I drove a racecar through your kitchen door',\n",
    "                    'I ate a hot dog at burger king and it was bad. I ate a burger at burger queen and it was very good']\n",
    "\n",
    "sample.fit(sunday_afternoon)\n",
    "text_data = sample.transform(sunday_afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# the 0th and 2nd index lines are very different, a number close to 0\n",
    "cosine_similarity(text_data[0],text_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91413793]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 0th and 3rd index lines are very similar, despite different lengths\n",
    "cosine_similarity(text_data[0],text_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

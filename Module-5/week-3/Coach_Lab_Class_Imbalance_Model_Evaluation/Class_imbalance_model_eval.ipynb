{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance and Model Evaluation/Comparison\n",
    "\n",
    "Agenda:\n",
    "- Review class imbalance\n",
    "- Review code for different ways to handle class imbalance\n",
    "- Review model evalutation metrics\n",
    "- Review how to compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know we have a class imbance problem???  We need to look at the target variable to see  if we have \"even\" groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(y, alpha=.80, color='blue')\n",
    "plt.title('Survivors vs Non-Survivors')\n",
    "plt.ylabel('# Passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above bar chart do we have the the same number of survivors vs. non-survivors?  What will happen if we run a model with this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a baseline model?  Why do we use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n",
    "\n",
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent', random_state=13).fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, dummy_pred))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, dummy_pred))\n",
    "\n",
    "#checking confusion matrix\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(dummy, classes=[0,1])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a classification model with class imbalance\n",
    "\n",
    "For the purpose of this example we are using a Logistic Regression classification mode but this will apply to other classification models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# dt_clf = DecisionTreeClassifier(max_depth=5)\n",
    "lr_clf = LogisticRegression(solver='liblinear', random_state=13)\n",
    "\n",
    "# dt_clf.fit(X_train, y_train)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred_test = dt_clf.predict(X_test)\n",
    "y_pred_test = lr_clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, y_pred_test))\n",
    "\n",
    "#checking confusion matrix\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(lr_clf, classes=[0,1])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data for handling class imbalances\n",
    "\n",
    "We are goign to change the training dataset to which we fit our model, so we want to bring our training data back together before we make those changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate our training data back together\n",
    "training  = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate minority and majority classes\n",
    "deceased = training[training.Survived==0]\n",
    "survived = training[training.Survived==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('deceased count: '+ str(len(deceased)))\n",
    "print('survived count: '+ str(len(survived)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](class_imb_images/resampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "survived_upsampled = resample(survived,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(deceased), # match number in majority class\n",
    "                          random_state=23) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([deceased, survived_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying logistic regression again with the balanced dataset\n",
    "y_train = upsampled.Survived\n",
    "X_train = upsampled.drop('Survived', axis=1)\n",
    "\n",
    "\n",
    "# upsampled_dt = DecisionTreeClassifier(max_depth=5)\n",
    "upsampled_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# upsampled_dt.fit(X_train, y_train)\n",
    "upsampled_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# upsampled_pred = upsampled_dt.predict(X_test)\n",
    "upsampled_pred = upsampled_lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, upsampled_pred))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, upsampled_pred))\n",
    "\n",
    "#checking confusion matrix\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(upsampled_lr, classes=[0,1])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# downsample majority\n",
    "survived_downsampled = resample(deceased,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(survived), # match minority n\n",
    "                                random_state = 23) # reproducible results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([survived_downsampled, survived])\n",
    "\n",
    "# checking counts\n",
    "downsampled.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying logistic regression again with the balanced dataset\n",
    "y_train = downsampled.Survived\n",
    "X_train = downsampled.drop('Survived', axis=1)\n",
    "\n",
    "\n",
    "# downsampled_dt = DecisionTreeClassifier(max_depth=5)\n",
    "downsampled_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# downsampled_dt.fit(X_train, y_train)\n",
    "downsampled_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# downsampled_pred = upsampled_dt.predict(X_test)\n",
    "downsampled_pred = downsampled_lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, downsampled_pred))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, downsampled_pred))\n",
    "\n",
    "#checking confusion matrix\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(downsampled_lr, classes=[0,1])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling: SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n",
    "\n",
    "![alt text](class_imb_images/smote.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n",
    "\n",
    "sm = SMOTE(random_state=23, ratio=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote_dt = DecisionTreeClassifier(max_depth=5)\n",
    "smote_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# smote_dt.fit(X_train, y_train)\n",
    "smote_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# smote_pred = smote_dt.predict(X_test)\n",
    "smote_pred = smote_lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, smote_pred))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, smote_pred))\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(smote_lr, classes=[0,1])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
